{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857a5f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import albumentations as albu\n",
    "import numpy as np\n",
    "import gc\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import CSVLogger\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, accuracy_score, f1_score\n",
    "from ModelArchitecture.DiceLoss import dice_metric_loss\n",
    "from ModelArchitecture import DUCK_Net\n",
    "from ImageLoader import ImageLoader2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6739f847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7852dbbf-f3a4-4db6-8c5e-98889bf3abd4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Checking the number of GPUs available\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9edbe667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model parameters\n",
    "\n",
    "img_size = 352\n",
    "dataset_type = 'kvasir' # Options: kvasir/cvc-clinicdb/cvc-colondb/etis-laribpolypdb\n",
    "learning_rate = 1e-4\n",
    "seed_value = 58800\n",
    "filters = 17 # Number of filters, the paper presents the results with 17 and 34\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "ct = datetime.now()\n",
    "\n",
    "model_type = \"DuckNet\"\n",
    "\n",
    "progress_path = 'ProgressFull/' + dataset_type + '_progress_csv_' + model_type + '_filters_' + str(filters) +  '_' + str(ct) + '.csv'\n",
    "progressfull_path = 'ProgressFull/' + dataset_type + '_progress_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.txt'\n",
    "plot_path = 'ProgressFull/' + dataset_type + '_progress_plot_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.png'\n",
    "model_path = 'ModelSaveTensorFlow/' + dataset_type + '/' + model_type + '_filters_' + str(filters) + '_' + str(ct)\n",
    "\n",
    "EPOCHS = 600\n",
    "min_loss_for_saving = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "264050d8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing training images and masks: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "# dataset_path = \"C:/Users/Jarvis/Desktop/ACAD/CV_419/project/datasets/Kvasir-SEG-20240205T070541Z-001/Kvasir-SEG/train\"\n",
    "X, Y = ImageLoader2D.load_data(img_size, img_size, -1, 'kvasir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ccf3a-371e-42c7-b7b0-607c0da78dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d69a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing training images and masks: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:11<00:00, 68.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 800 images and masks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Corrected paths for images and masks\n",
    "images_folder_path = \"C:/Users/Jarvis/Desktop/ACAD/CV_419/project/datasets/Kvasir-SEG/train/images/\"\n",
    "masks_folder_path = \"C:/Users/Jarvis/Desktop/ACAD/CV_419/project/datasets/Kvasir-SEG/train/masks/\"\n",
    "\n",
    "def load_data(img_height, img_width, images_to_be_loaded, dataset):\n",
    "    if dataset == \"kvasir\":\n",
    "        extension = \"*.jpg\"\n",
    "    elif dataset == \"cvc-clinicdb\":\n",
    "        extension = \"*.tif\"\n",
    "    elif dataset == \"cvc-colondb\" or dataset == \"etis-laribpolypdb\":\n",
    "        extension = \"*.png\"\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset type.\")\n",
    "\n",
    "    images_path = Path(images_folder_path)\n",
    "    masks_path = Path(masks_folder_path)\n",
    "\n",
    "    train_image_paths = list(images_path.glob(extension))\n",
    "    \n",
    "    if images_to_be_loaded == -1 or images_to_be_loaded > len(train_image_paths):\n",
    "        images_to_be_loaded = len(train_image_paths)\n",
    "\n",
    "    X_train = np.zeros((images_to_be_loaded, img_height, img_width, 3), dtype=np.float32)\n",
    "    Y_train = np.zeros((images_to_be_loaded, img_height, img_width), dtype=np.uint8)\n",
    "\n",
    "    print(f\"Resizing training images and masks: {images_to_be_loaded}\")\n",
    "    for n, image_path in tqdm(enumerate(train_image_paths), total=images_to_be_loaded):\n",
    "        if n >= images_to_be_loaded:\n",
    "            break\n",
    "\n",
    "        mask_path = masks_path / image_path.name\n",
    "\n",
    "        image = Image.open(image_path).resize((img_width, img_height))\n",
    "        image = np.array(image) / 255.0\n",
    "\n",
    "        mask = Image.open(mask_path).resize((img_width, img_height))\n",
    "        mask = np.array(mask)\n",
    "        mask = np.where(mask >= 127, 1, 0).astype(np.uint8)\n",
    "\n",
    "        X_train[n] = image\n",
    "        Y_train[n] = mask\n",
    "\n",
    "    Y_train = np.expand_dims(Y_train, axis=-1)\n",
    "\n",
    "    return X_train, Y_train\n",
    "\n",
    "# Example usage\n",
    "img_height, img_width = 352, 352\n",
    "images_to_be_loaded = -1 # Load all images\n",
    "dataset_type = 'kvasir'  # Options: 'kvasir', 'cvc-clinicdb', 'cvc-colondb', 'etis-laribpolypdb'\n",
    "\n",
    "X, Y = load_data(img_height, img_width, images_to_be_loaded, dataset_type)\n",
    "print(f\"Loaded {len(X)} images and masks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9cea6f3-346b-471f-98c4-1362dbfad8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75b6c029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Splitting the data, seed for reproducibility\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, shuffle= True, random_state = seed_value)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.111, shuffle= True, random_state = seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a94ede29-14d7-4b73-be3e-3e53dcfaaaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92a7b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the augmentations\n",
    "\n",
    "aug_train = albu.Compose([\n",
    "    albu.HorizontalFlip(),\n",
    "    albu.VerticalFlip(),\n",
    "    albu.ColorJitter(brightness=(0.6,1.6), contrast=0.2, saturation=0.1, hue=0.01, always_apply=True),\n",
    "    albu.Affine(scale=(0.5,1.5), translate_percent=(-0.125,0.125), rotate=(-180,180), shear=(-22.5,22), always_apply=True),\n",
    "])\n",
    "\n",
    "def augment_images():\n",
    "    x_train_out = []\n",
    "    y_train_out = []\n",
    "\n",
    "    for i in range (len(x_train)):\n",
    "        ug = aug_train(image=x_train[i], mask=y_train[i])\n",
    "        x_train_out.append(ug['image'])  \n",
    "        y_train_out.append(ug['mask'])\n",
    "\n",
    "    return np.array(x_train_out), np.array(y_train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1609dd32",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DUCK-Net\n"
     ]
    }
   ],
   "source": [
    "# Creating the model\n",
    "\n",
    "model = DUCK_Net.create_model(img_height=img_size, img_width=img_size, input_chanels=3, out_classes=1, starting_filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e513d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=dice_metric_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef712b9",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, epoch 0\n",
      "Learning Rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "step = 0\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    \n",
    "    print(f'Training, epoch {epoch}')\n",
    "    print('Learning Rate: ' + str(learning_rate))\n",
    "\n",
    "    step += 1\n",
    "        \n",
    "    image_augmented, mask_augmented = augment_images()\n",
    "    \n",
    "    csv_logger = CSVLogger(progress_path, append=True, separator=';')\n",
    "    \n",
    "    model.fit(x=image_augmented, y=mask_augmented, epochs=1, batch_size=1, validation_data=(x_valid, y_valid), verbose=1, callbacks=[csv_logger])\n",
    "    \n",
    "    prediction_valid = model.predict(x_valid, verbose=0)\n",
    "    loss_valid = dice_metric_loss(y_valid, prediction_valid)\n",
    "    \n",
    "    loss_valid = loss_valid.numpy()\n",
    "    print(\"Loss Validation: \" + str(loss_valid))\n",
    "        \n",
    "    prediction_test = model.predict(x_test, verbose=0)\n",
    "    loss_test = dice_metric_loss(y_test, prediction_test)\n",
    "    loss_test = loss_test.numpy()\n",
    "    print(\"Loss Test: \" + str(loss_test))\n",
    "        \n",
    "    with open(progressfull_path, 'a') as f:\n",
    "        f.write('epoch: ' + str(epoch) + '\\nval_loss: ' + str(loss_valid) + '\\ntest_loss: ' + str(loss_test) + '\\n\\n\\n')\n",
    "    \n",
    "    if min_loss_for_saving > loss_valid:\n",
    "        min_loss_for_saving = loss_valid\n",
    "        print(\"Saved model with val_loss: \", loss_valid)\n",
    "        model.save(model_path)\n",
    "        \n",
    "    del image_augmented\n",
    "    del mask_augmented\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd52447",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Computing the metrics and saving the results\n",
    "\n",
    "print(\"Loading the model\")\n",
    "\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={'dice_metric_loss':dice_metric_loss})\n",
    "\n",
    "prediction_train = model.predict(x_train, batch_size=4)\n",
    "prediction_valid = model.predict(x_valid, batch_size=4)\n",
    "prediction_test = model.predict(x_test, batch_size=4)\n",
    "\n",
    "print(\"Predictions done\")\n",
    "\n",
    "dice_train = f1_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_train > 0.5))\n",
    "dice_test = f1_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                          np.ndarray.flatten(prediction_test > 0.5))\n",
    "dice_valid = f1_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Dice finished\")\n",
    "\n",
    "\n",
    "miou_train = jaccard_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_train > 0.5))\n",
    "miou_test = jaccard_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                          np.ndarray.flatten(prediction_test > 0.5))\n",
    "miou_valid = jaccard_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Miou finished\")\n",
    "\n",
    "\n",
    "precision_train = precision_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                                  np.ndarray.flatten(prediction_train > 0.5))\n",
    "precision_test = precision_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                                 np.ndarray.flatten(prediction_test > 0.5))\n",
    "precision_valid = precision_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                                  np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Precision finished\")\n",
    "\n",
    "\n",
    "recall_train = recall_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                            np.ndarray.flatten(prediction_train > 0.5))\n",
    "recall_test = recall_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_test > 0.5))\n",
    "recall_valid = recall_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                            np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Recall finished\")\n",
    "\n",
    "\n",
    "accuracy_train = accuracy_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                                np.ndarray.flatten(prediction_train > 0.5))\n",
    "accuracy_test = accuracy_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                               np.ndarray.flatten(prediction_test > 0.5))\n",
    "accuracy_valid = accuracy_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                                np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "\n",
    "print(\"Accuracy finished\")\n",
    "\n",
    "\n",
    "final_file = 'results_' + model_type + '_' + str(filters) + '_' + dataset_type + '.txt'\n",
    "print(final_file)\n",
    "\n",
    "with open(final_file, 'a') as f:\n",
    "    f.write(dataset_type + '\\n\\n')\n",
    "    f.write('dice_train: ' + str(dice_train) + ' dice_valid: ' + str(dice_valid) + ' dice_test: ' + str(dice_test) + '\\n\\n')\n",
    "    f.write('miou_train: ' + str(miou_train) + ' miou_valid: ' + str(miou_valid) + ' miou_test: ' + str(miou_test) + '\\n\\n')\n",
    "    f.write('precision_train: ' + str(precision_train) + ' precision_valid: ' + str(precision_valid) + ' precision_test: ' + str(precision_test) + '\\n\\n')\n",
    "    f.write('recall_train: ' + str(recall_train) + ' recall_valid: ' + str(recall_valid) + ' recall_test: ' + str(recall_test) + '\\n\\n')\n",
    "    f.write('accuracy_train: ' + str(accuracy_train) + ' accuracy_valid: ' + str(accuracy_valid) + ' accuracy_test: ' + str(accuracy_test) + '\\n\\n\\n\\n')\n",
    "\n",
    "print('File done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
