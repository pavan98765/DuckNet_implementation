{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857a5f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import albumentations as albu\n",
    "import numpy as np\n",
    "import gc\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import CSVLogger\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, accuracy_score, f1_score\n",
    "from ModelArchitecture.DiceLoss import dice_metric_loss\n",
    "from ModelArchitecture import DUCK_Net\n",
    "from ImageLoader import ImageLoader2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6739f847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7852dbbf-f3a4-4db6-8c5e-98889bf3abd4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Checking the number of GPUs available\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9edbe667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model parameters\n",
    "\n",
    "img_size = 352\n",
    "dataset_type = 'kvasir' # Options: kvasir/cvc-clinicdb/cvc-colondb/etis-laribpolypdb\n",
    "learning_rate = 1e-4\n",
    "seed_value = 58800\n",
    "filters = 17 # Number of filters, the paper presents the results with 17 and 34\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "ct = datetime.now()\n",
    "\n",
    "model_type = \"DuckNet\"\n",
    "\n",
    "progress_path = 'ProgressFull/' + dataset_type + '_progress_csv_' + model_type + '_filters_' + str(filters) +  '_' + str(ct) + '.csv'\n",
    "progressfull_path = 'ProgressFull/' + dataset_type + '_progress_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.txt'\n",
    "plot_path = 'ProgressFull/' + dataset_type + '_progress_plot_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.png'\n",
    "model_path = 'ModelSaveTensorFlow/' + dataset_type + '/' + model_type + '_filters_' + str(filters) + '_' + str(ct)\n",
    "\n",
    "EPOCHS = 600\n",
    "min_loss_for_saving = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "264050d8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing training images and masks: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "dataset_path = \"C:/Users/Jarvis/Desktop/ACAD/CV_419/project/datasets/Kvasir-SEG-20240205T070541Z-001/Kvasir-SEG/train\"\n",
    "X, Y = ImageLoader2D.load_data(img_size, img_size, -1, 'kvasir')\n",
    "# X, Y = ImageLoader2D.load_data(img_size, img_size, -1, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d30c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def load_image_and_mask(image_path, mask_path, img_height, img_width):\n",
    "    def _load_image_and_mask(image_path, mask_path):\n",
    "        # Load and resize the image\n",
    "        image = load_img(image_path.decode(), target_size=(img_height, img_width))\n",
    "        image = img_to_array(image) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "        # Load and resize the mask\n",
    "        mask = load_img(mask_path.decode(), target_size=(img_height, img_width), color_mode=\"grayscale\")\n",
    "        mask = img_to_array(mask) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "        # Convert mask to binary (0 or 1)\n",
    "        mask[mask > 0.5] = 1\n",
    "        mask[mask <= 0.5] = 0\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    [image, mask] = tf.py_function(_load_image_and_mask, [image_path, mask_path], [tf.float32, tf.float32])\n",
    "    image.set_shape((img_height, img_width, 3))\n",
    "    mask.set_shape((img_height, img_width, 1))\n",
    "\n",
    "    return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d69a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(images_directory, masks_directory, img_height, img_width):\n",
    "    image_paths = tf.data.Dataset.list_files(images_directory + \"*.jpg\", shuffle=False)\n",
    "    mask_paths = tf.data.Dataset.list_files(masks_directory + \"*.jpg\", shuffle=False)  # Adjust file extension if needed\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((image_paths, mask_paths))\n",
    "    dataset = dataset.map(lambda x, y: load_image_and_mask(x, y, img_height, img_width), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2371c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_directory = 'C:/Users/Jarvis/Desktop/ACAD/CV_419/project/datasets/Kvasir-SEG-20240205T070541Z-001/Kvasir-SEG/train/images/'\n",
    "masks_directory = 'C:/Users/Jarvis/Desktop/ACAD/CV_419/project/datasets/Kvasir-SEG-20240205T070541Z-001/Kvasir-SEG/train/masks/'\n",
    "\n",
    "img_height, img_width = 256, 256  # Adjust the size as needed\n",
    "\n",
    "dataset = create_dataset(images_directory, masks_directory, img_height, img_width)\n",
    "\n",
    "# You can now use 'dataset' in your training pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89c9bf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset element_spec=(TensorSpec(shape=(256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(256, 256, 1), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75b6c029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Splitting the data, seed for reproducibility\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseed_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m x_train, x_valid, y_train, y_valid \u001b[38;5;241m=\u001b[39m train_test_split(x_train, y_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.111\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, random_state \u001b[38;5;241m=\u001b[39m seed_value)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cvproject\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2433\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2430\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2432\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2433\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cvproject\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2111\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2108\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2115\u001b[0m     )\n\u001b[0;32m   2117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Splitting the data, seed for reproducibility\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, shuffle= True, random_state = seed_value)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.111, shuffle= True, random_state = seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92a7b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the augmentations\n",
    "\n",
    "aug_train = albu.Compose([\n",
    "    albu.HorizontalFlip(),\n",
    "    albu.VerticalFlip(),\n",
    "    albu.ColorJitter(brightness=(0.6,1.6), contrast=0.2, saturation=0.1, hue=0.01, always_apply=True),\n",
    "    albu.Affine(scale=(0.5,1.5), translate_percent=(-0.125,0.125), rotate=(-180,180), shear=(-22.5,22), always_apply=True),\n",
    "])\n",
    "\n",
    "def augment_images():\n",
    "    x_train_out = []\n",
    "    y_train_out = []\n",
    "\n",
    "    for i in range (len(x_train)):\n",
    "        ug = aug_train(image=x_train[i], mask=y_train[i])\n",
    "        x_train_out.append(ug['image'])  \n",
    "        y_train_out.append(ug['mask'])\n",
    "\n",
    "    return np.array(x_train_out), np.array(y_train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1609dd32",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DUCK-Net\n"
     ]
    }
   ],
   "source": [
    "# Creating the model\n",
    "\n",
    "model = DUCK_Net.create_model(img_height=img_size, img_width=img_size, input_chanels=3, out_classes=1, starting_filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e513d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=dice_metric_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ef712b9",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, epoch 0\n",
      "Learning Rate: 0.0001\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearning Rate: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(learning_rate))\n\u001b[0;32m     10\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 12\u001b[0m image_augmented, mask_augmented \u001b[38;5;241m=\u001b[39m \u001b[43maugment_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m csv_logger \u001b[38;5;241m=\u001b[39m CSVLogger(progress_path, append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mimage_augmented, y\u001b[38;5;241m=\u001b[39mmask_augmented, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(x_valid, y_valid), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[csv_logger])\n",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m, in \u001b[0;36maugment_images\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m x_train_out \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m y_train_out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[43mx_train\u001b[49m)):\n\u001b[0;32m     15\u001b[0m     ug \u001b[38;5;241m=\u001b[39m aug_train(image\u001b[38;5;241m=\u001b[39mx_train[i], mask\u001b[38;5;241m=\u001b[39my_train[i])\n\u001b[0;32m     16\u001b[0m     x_train_out\u001b[38;5;241m.\u001b[39mappend(ug[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m])  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "step = 0\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    \n",
    "    print(f'Training, epoch {epoch}')\n",
    "    print('Learning Rate: ' + str(learning_rate))\n",
    "\n",
    "    step += 1\n",
    "        \n",
    "    image_augmented, mask_augmented = augment_images()\n",
    "    \n",
    "    csv_logger = CSVLogger(progress_path, append=True, separator=';')\n",
    "    \n",
    "    model.fit(x=image_augmented, y=mask_augmented, epochs=1, batch_size=4, validation_data=(x_valid, y_valid), verbose=1, callbacks=[csv_logger])\n",
    "    \n",
    "    prediction_valid = model.predict(x_valid, verbose=0)\n",
    "    loss_valid = dice_metric_loss(y_valid, prediction_valid)\n",
    "    \n",
    "    loss_valid = loss_valid.numpy()\n",
    "    print(\"Loss Validation: \" + str(loss_valid))\n",
    "        \n",
    "    prediction_test = model.predict(x_test, verbose=0)\n",
    "    loss_test = dice_metric_loss(y_test, prediction_test)\n",
    "    loss_test = loss_test.numpy()\n",
    "    print(\"Loss Test: \" + str(loss_test))\n",
    "        \n",
    "    with open(progressfull_path, 'a') as f:\n",
    "        f.write('epoch: ' + str(epoch) + '\\nval_loss: ' + str(loss_valid) + '\\ntest_loss: ' + str(loss_test) + '\\n\\n\\n')\n",
    "    \n",
    "    if min_loss_for_saving > loss_valid:\n",
    "        min_loss_for_saving = loss_valid\n",
    "        print(\"Saved model with val_loss: \", loss_valid)\n",
    "        model.save(model_path)\n",
    "        \n",
    "    del image_augmented\n",
    "    del mask_augmented\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd52447",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Computing the metrics and saving the results\n",
    "\n",
    "print(\"Loading the model\")\n",
    "\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={'dice_metric_loss':dice_metric_loss})\n",
    "\n",
    "prediction_train = model.predict(x_train, batch_size=4)\n",
    "prediction_valid = model.predict(x_valid, batch_size=4)\n",
    "prediction_test = model.predict(x_test, batch_size=4)\n",
    "\n",
    "print(\"Predictions done\")\n",
    "\n",
    "dice_train = f1_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_train > 0.5))\n",
    "dice_test = f1_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                          np.ndarray.flatten(prediction_test > 0.5))\n",
    "dice_valid = f1_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Dice finished\")\n",
    "\n",
    "\n",
    "miou_train = jaccard_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_train > 0.5))\n",
    "miou_test = jaccard_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                          np.ndarray.flatten(prediction_test > 0.5))\n",
    "miou_valid = jaccard_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Miou finished\")\n",
    "\n",
    "\n",
    "precision_train = precision_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                                  np.ndarray.flatten(prediction_train > 0.5))\n",
    "precision_test = precision_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                                 np.ndarray.flatten(prediction_test > 0.5))\n",
    "precision_valid = precision_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                                  np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Precision finished\")\n",
    "\n",
    "\n",
    "recall_train = recall_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                            np.ndarray.flatten(prediction_train > 0.5))\n",
    "recall_test = recall_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_test > 0.5))\n",
    "recall_valid = recall_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                            np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Recall finished\")\n",
    "\n",
    "\n",
    "accuracy_train = accuracy_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                                np.ndarray.flatten(prediction_train > 0.5))\n",
    "accuracy_test = accuracy_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                               np.ndarray.flatten(prediction_test > 0.5))\n",
    "accuracy_valid = accuracy_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                                np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "\n",
    "print(\"Accuracy finished\")\n",
    "\n",
    "\n",
    "final_file = 'results_' + model_type + '_' + str(filters) + '_' + dataset_type + '.txt'\n",
    "print(final_file)\n",
    "\n",
    "with open(final_file, 'a') as f:\n",
    "    f.write(dataset_type + '\\n\\n')\n",
    "    f.write('dice_train: ' + str(dice_train) + ' dice_valid: ' + str(dice_valid) + ' dice_test: ' + str(dice_test) + '\\n\\n')\n",
    "    f.write('miou_train: ' + str(miou_train) + ' miou_valid: ' + str(miou_valid) + ' miou_test: ' + str(miou_test) + '\\n\\n')\n",
    "    f.write('precision_train: ' + str(precision_train) + ' precision_valid: ' + str(precision_valid) + ' precision_test: ' + str(precision_test) + '\\n\\n')\n",
    "    f.write('recall_train: ' + str(recall_train) + ' recall_valid: ' + str(recall_valid) + ' recall_test: ' + str(recall_test) + '\\n\\n')\n",
    "    f.write('accuracy_train: ' + str(accuracy_train) + ' accuracy_valid: ' + str(accuracy_valid) + ' accuracy_test: ' + str(accuracy_test) + '\\n\\n\\n\\n')\n",
    "\n",
    "print('File done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
